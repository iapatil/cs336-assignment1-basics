{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[115, 111, 109, 101]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Unicode -> Defines ~155k chars across 168 scripts\n",
    "# Python -> ord() -> Gives unicode integer representation for each char, chr() -> converts integer unicode code point to coressponding char\n",
    "# \n",
    "# Impractical to train tokenizer directly on these code points -> very large vocab, and very sparse\n",
    "# \n",
    "# Instead use unicode encoding -> convert unicode char into sequence of bytes (UTF-8, UTF-16, UTF-32)\n",
    "# UTF-8 -> dominant encoding for the Internet (vocab size of 256 more manageable)\n",
    "# When using byte-level tokenization, don't have to worry about OOV tokens, since any input text can be expressed as sequence of integers from 0-255\n",
    "\n",
    "a = out[0].encode(\"utf-8\") # -> converts to Python bytes object\n",
    "list(a) # convert string into sequence of bytes\n",
    "# If you just prepend a string with  b\" for eg. b\"Hello There\", you get the sequence of bytes for this string where each ASCII unicode character is converted to a single byte, however this does not work for non ASCII-characters that might map to more than\n",
    "# one byte. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'some'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = bytes(list(a)).decode(\"utf-8\") # or a.decode(\"utf-8\")\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subword tokenization -> midpoint between word-level tokenization and byte-level tokenization, trades off a slightly bigger vocab size for a better compression of input byte sequence. Byte-level tokenization has vocab size of 256, sub-word is slightly higher with\n",
    "# byte pairs frequently appearing together in input as additional vocab entries.\n",
    "\n",
    "# Byte-level BPE tokenizer -> Vocab items are bytes, or merges sequence of bytes -> gives best of both worlds in terms of out-of-vocab handling and mangeable input sequence length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-tokeization\n",
    "\"\"\"\n",
    "\n",
    "import regex as re\n",
    "\n",
    "PAT = r\"\"\"'(?:[sdmt]|ll|ve|re)| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+\"\"\"\n",
    "out = re.findall(PAT, \"some text that i'll pre-tokenize\") # re.finditer is preferable, as we don't have to store all pre-tokeizer words, it's an iterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computing BPE Merges\n",
    "\"\"\"\n",
    "\n",
    "# Prefer lexicographically greater pair, when deciding between pairs with same frequency on which to merge.\n",
    "# For efficiency during BPE training -> don't consider pairs that cross pre-token boundaries\n",
    "# Some tokens should be never split into multiple tokens -> eg. end_of_text (should be added to vocab so they have fixed token ID)\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'<|endoftext|>'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b\"<|endoftext|>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 124, 101, 110, 100, 111, 102, 116, 101, 120, 116, 124, 62]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(b\"<|endoftext|>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"<|endoftext|>\".encode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[60, 124, 101, 110, 100, 111, 102, 116, 101, 120, 116, 124, 62]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs336",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
